{
  "timestamp": "2025-09-26T18:43:46.981300",
  "hypothesis": "GQA structure determines even/odd pattern emergence",
  "predictions": {
    "EleutherAI/pythia-160m": {
      "model_name": "EleutherAI/pythia-160m",
      "model_type": "gpt_neox",
      "n_heads": 12,
      "n_layers": 12,
      "hidden_size": 768,
      "gqa_structure": {
        "has_gqa": false,
        "heads_per_group": 12,
        "num_groups": 1
      },
      "predicted_pattern": "even_odd_works"
    },
    "EleutherAI/pythia-410m": {
      "model_name": "EleutherAI/pythia-410m",
      "model_type": "gpt_neox",
      "n_heads": 16,
      "n_layers": 24,
      "hidden_size": 1024,
      "gqa_structure": {
        "has_gqa": false,
        "heads_per_group": 16,
        "num_groups": 1
      },
      "predicted_pattern": "even_odd_works"
    },
    "microsoft/DialoGPT-small": {
      "model_name": "microsoft/DialoGPT-small",
      "model_type": "gpt2",
      "n_heads": 12,
      "n_layers": 12,
      "hidden_size": 768,
      "gqa_structure": {
        "has_gqa": false,
        "heads_per_group": 12,
        "num_groups": 1
      },
      "predicted_pattern": "even_odd_works"
    },
    "meta-llama/Meta-Llama-3.1-8B-Instruct": {
      "model_name": "meta-llama/Meta-Llama-3.1-8B-Instruct",
      "model_type": "llama",
      "n_heads": 32,
      "n_layers": 32,
      "hidden_size": 4096,
      "gqa_structure": {
        "has_gqa": true,
        "num_kv_heads": 8,
        "num_query_heads": 32,
        "heads_per_group": 4,
        "num_groups": 8
      },
      "predicted_pattern": "even_odd_works"
    },
    "meta-llama/Llama-2-7b-hf": {
      "model_name": "meta-llama/Llama-2-7b-hf",
      "model_type": "llama",
      "n_heads": 32,
      "n_layers": 32,
      "hidden_size": 4096,
      "gqa_structure": {
        "has_gqa": true,
        "num_kv_heads": 32,
        "num_query_heads": 32,
        "heads_per_group": 1,
        "num_groups": 32
      },
      "predicted_pattern": "even_odd_fails"
    },
    "google/gemma-2b": {
      "model_name": "google/gemma-2b",
      "model_type": "gemma",
      "n_heads": 8,
      "n_layers": 18,
      "hidden_size": 2048,
      "gqa_structure": {
        "has_gqa": true,
        "num_kv_heads": 1,
        "num_query_heads": 8,
        "heads_per_group": 8,
        "num_groups": 1
      },
      "predicted_pattern": "even_odd_works"
    },
    "google/gemma-7b": {
      "model_name": "google/gemma-7b",
      "model_type": "gemma",
      "n_heads": 16,
      "n_layers": 28,
      "hidden_size": 3072,
      "gqa_structure": {
        "has_gqa": true,
        "num_kv_heads": 16,
        "num_query_heads": 16,
        "heads_per_group": 1,
        "num_groups": 16
      },
      "predicted_pattern": "even_odd_fails"
    }
  },
  "test_results": {
    "EleutherAI/pythia-160m": {
      "model_name": "EleutherAI/pythia-160m",
      "n_heads": 12,
      "test_layer": 6,
      "task": "9.8_vs_9.11",
      "even_success_rate": 0.0,
      "odd_success_rate": 0.0,
      "pattern_detected": false,
      "error": null,
      "advantage": "none"
    },
    "meta-llama/Meta-Llama-3.1-8B-Instruct": {
      "model_name": "meta-llama/Meta-Llama-3.1-8B-Instruct",
      "n_heads": 32,
      "test_layer": 16,
      "task": "9.8_vs_9.11",
      "even_success_rate": 0.0,
      "odd_success_rate": 0.0,
      "pattern_detected": false,
      "error": null,
      "advantage": "none"
    },
    "google/gemma-2b": {
      "model_name": "google/gemma-2b",
      "n_heads": 8,
      "test_layer": 9,
      "task": "9.8_vs_9.11",
      "even_success_rate": 0.0,
      "odd_success_rate": 0.0,
      "pattern_detected": false,
      "error": null,
      "advantage": "none"
    }
  },
  "validation": {
    "correct_predictions": 0,
    "total_predictions": 0,
    "hypothesis_supported": false,
    "details": {}
  },
  "recommendations": {
    "immediate_tests": [
      "Test Llama-2-7b-hf to confirm moderate GQA pattern",
      "Test pythia-410m to confirm standard MHA pattern",
      "Test additional Gemma models to confirm extreme GQA pattern"
    ],
    "extended_validation": [
      "Test numerical tasks beyond decimal comparison",
      "Investigate GQA group boundaries with targeted patching",
      "Test models with 2-3 heads per GQA group (edge cases)",
      "Cross-validate with other architectural features"
    ],
    "theoretical_follow_up": [
      "Analyze training dynamics that lead to specialization",
      "Test if artificial GQA grouping affects specialization",
      "Investigate other tasks that show even/odd patterns"
    ]
  }
}